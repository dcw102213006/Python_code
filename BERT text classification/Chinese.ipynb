{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,TFBertModel,BertConfig\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#Training\n",
    "with open('../Patent_data/labels.pickle', 'rb') as file:\n",
    "    labels =pickle.load(file)\n",
    "with open('../Patent_data/tokens.pickle', 'rb') as file:\n",
    "    tokens =pickle.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized ['用', '於', '半', '導', '體', '裝', '置', '之', '黏', '合', '劑', '膜', '本', '發', '明', '提', '供', '了', '一', '種', '用', '於', '半', '導', '體', '裝', '置', '的', '黏', '合', '劑', '膜', '，', '所', '述', '黏', '合', '劑', '膜', '包', '括', '在', '0', '℃', '至', '5', '℃', '下', '具', '有', '50', 'μ', 'm', '/', 'm', '‧', '℃', '至', '150', 'μ', 'm', '/', 'm', '‧', '℃', '的', '線', '性', '膨', '脹', '係', '數', '的', '基', '膜', '。', '所', '述', '黏', '合', '劑', '膜', '在', '低', '溫', '儲', '存', '較', '長', '時', '間', '後', '具', '有', '優', '異', '的', '捲', '繞', '形', '狀', '穩', '定', '性', '，', '從', '而', '在', '以', '後', '的', '半', '導', '體', '封', '裝', '工', '藝', '中', '不', '會', '導', '致', '傾', '斜', '現', '象', '並', '降', '低', '缺', '陷', '。']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "max_length_test = 20\n",
    "test_sentence = tokens[0]\n",
    "\n",
    "tokenized = tokenizer.tokenize(test_sentence)\n",
    "print('tokenized', tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded {'input_ids': [101, 4500, 3176, 1288, 2206, 7768, 6172, 5390, 722, 7945, 1394, 1212, 5606, 3315, 4634, 3209, 2990, 897, 749, 671, 4934, 4500, 3176, 1288, 2206, 7768, 6172, 5390, 4638, 7945, 1394, 1212, 5606, 8024, 2792, 6835, 7945, 1394, 1212, 5606, 1259, 2886, 1762, 121, 360, 5635, 126, 360, 678, 1072, 3300, 8145, 220, 155, 120, 155, 342, 360, 5635, 8269, 220, 155, 120, 155, 342, 360, 4638, 5221, 2595, 5610, 5568, 913, 3149, 4638, 1825, 5606, 511, 2792, 6835, 7945, 1394, 1212, 5606, 1762, 856, 3984, 1033, 2100, 6733, 7269, 3229, 7279, 2527, 1072, 3300, 1032, 4530, 4638, 2947, 5254, 2501, 4311, 4952, 2137, 2595, 8024, 2537, 5445, 1762, 809, 2527, 4638, 1288, 2206, 7768, 2196, 6172, 2339, 5971, 704, 679, 3298, 2206, 5636, 1005, 3162, 4412, 6496, 699, 7360, 856, 5375, 7379, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "bert_input = tokenizer.encode_plus(\n",
    "                        test_sentence,                      \n",
    "                        add_special_tokens = True, # add [CLS], [SEP]\n",
    "                        max_length = 150, # max length of the text that can go to BERT\n",
    "                        pad_to_max_length = True, # add [PAD] tokens\n",
    "                        return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "              )\n",
    "print('encoded', bert_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS],用,於,半,導,體,裝,置,之,黏,合,劑,膜,本,發,明,提,供,了,一,種,用,於,半,導,體,裝,置,的,黏,合,劑,膜,，,所,述,黏,合,劑,膜,包,括,在,0,℃,至,5,℃,下,具,有,50,μ,m,/,m,‧,℃,至,150,μ,m,/,m,‧,℃,的,線,性,膨,脹,係,數,的,基,膜,。,所,述,黏,合,劑,膜,在,低,溫,儲,存,較,長,時,間,後,具,有,優,異,的,捲,繞,形,狀,穩,定,性,，,從,而,在,以,後,的,半,導,體,封,裝,工,藝,中,不,會,導,致,傾,斜,現,象,並,降,低,缺,陷,。,[SEP],[PAD],[PAD],[PAD],[PAD],[PAD],[PAD],[PAD],[PAD],[PAD],[PAD],[PAD],[PAD],[PAD],[PAD],[PAD],"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.vocab\n",
    "for index in bert_input['input_ids']:\n",
    "    print(list(vocab)[index],end=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述範例是設定字數=150字:\n",
    "* 若文件不足150字就會以[SEP]結尾並padding至150\n",
    "* 若文件超過150字，多餘的字數將不輸入，句尾(第151字)放置[SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFBertModel.from_pretrained(\"bert-base-chinese\", output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拿hidden states:  \n",
    "+ outputs[2]是return hidden_states\n",
    "+ bert-base-chinese hiddenstates維度13 * batch * max_length * 768  \n",
    "+ 此範例是13 * 1 * 150 * 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_input = tokenizer.encode_plus(\n",
    "                    tokens[0],                      \n",
    "                    add_special_tokens = True, # add [CLS], [SEP]\n",
    "                    max_length = 150, # max length of the text that can go to BERT\n",
    "                    pad_to_max_length = True, # add [PAD] tokens\n",
    "                    return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "          )\n",
    "input_ids = tf.constant(bert_input['input_ids'])[None, :]\n",
    "outputs =  model(input_ids)\n",
    "hidden_states = outputs[2]  #hidden_states dimension:[# layers, # batches, # tokens, # features]\n",
    "hidden_states = tf.reshape(hidden_states,[13,-1,768]) #dimension:[# layers, # tokens, # features]\n",
    "avg_doc_vector = tf.math.reduce_mean(hidden_states[-2],axis=0)\n",
    "avg_doc_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg_doc_vector拿到hidden_states倒數第二層Encoder的向量"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
